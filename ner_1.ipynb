{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ner_1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOepGJUgeAJ3JN838rTGhFX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"pYHb2JEW53-P"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B_B0qsfBqS9H"},"source":["! pip install tqdm\n","! pip install overrides\n","! pip install torch==1.7.0\n","! pip install transformers==3.5.1\n","! pip install pytorch-lightning==1.1.0\n","! wget https://github.com/KLUE-benchmark/KLUE/raw/main/klue_benchmark/klue-ner-v1.1/klue-ner-v1.1_train.tsv\n","! wget https://github.com/KLUE-benchmark/KLUE/raw/main/klue_benchmark/klue-ner-v1.1/klue-ner-v1.1_dev.tsv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gFjX4naNqUVn"},"source":["import re\n","import argparse\n","import logging\n","from pathlib import Path\n","from typing import Dict, List, Optional, Union\n","from torch.utils.data import TensorDataset\n","from dataclasses import dataclass\n","\n","import pytorch_lightning as pl\n","import torch\n","import torch.nn as nn\n","\n","# huggingface's transformers library\n","from transformers import AdamW, AutoConfig, AutoTokenizer, PretrainedConfig, PreTrainedTokenizer ,AutoModelForTokenClassification\n","from transformers.optimization import (\n","    Adafactor,\n","    get_cosine_schedule_with_warmup,\n","    get_cosine_with_hard_restarts_schedule_with_warmup,\n","    get_linear_schedule_with_warmup,\n","    get_polynomial_decay_schedule_with_warmup,\n",")\n","\n","# the tqdm library used to show the iteration progress\n","import tqdm\n","tqdmn = tqdm.notebook.tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WArhx5jcqWQ8"},"source":["roberta_version = 'klue/roberta-large'\n","tokenizer = AutoTokenizer.from_pretrained(roberta_version)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KXTh-XukqX11"},"source":["logger = logging.getLogger(__name__)\n","\n","tokenizer_type =\"bert-wp\"\n","hparams = argparse.Namespace(max_seq_length=128)\n","\n","\n","@dataclass\n","class InputExample:\n","    \"\"\"A single example of data.utils\n","    This is for YNAT, KLUE-NLI, KLUE-NER, and KLUE-RE.\n","    Args:\n","        guid: Unique id for the example.\n","        text_a: string. The untokenized text of the first sequence. For single\n","            sequence tasks, only this sequence must be specified.\n","        text_b: (Optional) string. The untokenized text of the second sequence.\n","            Only must be specified for sequence pair tasks.\n","        label: (Optional) string. The label of the example. This should be\n","            specified for train and dev examples, but not for test examples.\n","    \"\"\"\n","\n","    guid: str\n","    text_a: str\n","    text_b: Optional[str] = None\n","    label: Optional[str] = None\n","\n","    def to_dict(self) -> Dict[str, str]:\n","        return dataclasses.asdict(self)\n","\n","    def to_json_string(self) -> None:\n","        \"\"\"Serializes this instance to a JSON string.\"\"\"\n","        return json.dumps(self.to_dict(), indent=2) + \"\\n\"\n","\n","@dataclass(frozen=True)\n","class InputFeatures:\n","    \"\"\"A single set of features of data to feed into the pretrained models.\n","    This is for YNAT, KLUE-STS, KLUE-NLI, KLUE-NER, and KLUE-RE. Property names\n","    are same with the corresponding inputs to a model.\n","    Args:\n","        input_ids: Indices of input sequence tokens in the vocabulary.\n","        attention_mask: Mask to avoid performing attention on padding token indices.\n","            Mask values selected in ``[0, 1]``: Usually ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded)\n","            tokens.\n","        token_type_ids: (Optional) Segment token indices to indicate first and second\n","            portions of the inputs. Only some models use them.\n","        label: (Optional) Label corresponding to the input. Int for classification problems,\n","            float for regression problems.\n","    \"\"\"\n","\n","    input_ids: List[int]\n","    attention_mask: Optional[List[int]] = None\n","    token_type_ids: Optional[List[int]] = None\n","    label: Optional[Union[int, float]] = None\n","\n","    def to_json_string(self) -> None:\n","        \"\"\"Serializes this instance to a JSON string.\"\"\"\n","        return json.dumps(dataclasses.asdict(self)) + \"\\n\"\n","\n","def convert_examples_to_features(\n","    examples: List[InputExample],\n","    tokenizer: \"klue-ner-v1.1_train.tsv\",\n","    label_list: List[str],\n","    max_length: Optional[int] = None,\n","    task_mode: Optional[str] = None,\n",") -> List[InputFeatures]:\n","    \"\"\"Converts dataset in InputExample to dataset in InputFeatures to feed into pretrained models.\n","    This is for YNAT, KLUE-STS, KLUE-NLI, and KLUE-NER.\n","    Args:\n","        examples: List of InputExample converted from the raw dataset.\n","        tokenizer: Tokenizer of the pretrained model.\n","        label_list: List of labels of the task.\n","        max_length: Maximum length of the input tokens.\n","        task_mode: Task type.\n","    Returns:\n","        features: List of InputFeatures for the task and model.\n","    \"\"\"\n","    if max_length is None:\n","        max_length = tokenizer.max_len\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    def label_from_example(example: InputExample) -> Union[int, float, None, List[int]]:\n","        if example.label is None:\n","            return None\n","        if task_mode == \"classification\":\n","            return label_map[example.label]\n","        elif task_mode == \"regression\":\n","            return float(example.label)\n","        elif task_mode == \"tagging\":  # See KLUE paper: https://arxiv.org/pdf/2105.09680.pdf\n","            token_label = [label_map[\"O\"]] * (max_length)\n","            for i, label in enumerate(example.label[: max_length - 2]):  # last [SEP] label -> 'O'\n","                token_label[i + 1] = label_map[label]  # first [CLS] label -> 'O'\n","            return token_label\n","        raise KeyError(task_mode)\n","\n","    labels = [label_from_example(example) for example in examples]\n","\n","    batch_encoding = tokenizer(\n","        [(example.text_a, example.text_b) for example in examples],\n","        max_length=max_length,\n","        padding=\"max_length\",\n","        truncation=True,\n","    )\n","\n","    features = []\n","    for i in range(len(examples)):\n","        inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n","\n","        feature = InputFeatures(**inputs, label=labels[i])\n","        features.append(feature)\n","\n","    for i, example in enumerate(examples[:5]):\n","        logger.info(\"*** Example ***\")\n","        logger.info(\"guid: %s\" % (example.guid))\n","        logger.info(\"features: %s\" % features[i])\n","\n","    return features\n","\n","\n","\n","def get_labels() -> List[str]:\n","    return [\"B-PS\", \"I-PS\", \"B-LC\", \"I-LC\", \"B-OG\", \"I-OG\", \"B-DT\", \"I-DT\", \"B-TI\", \"I-TI\", \"B-QT\", \"I-QT\", \"O\"]\n","\n","def _create_examples(file_path: str, dataset_type: str) -> List[InputExample]:\n","    \"\"\"Loads the raw dataset and converts to InputExample.\n","    Since the ner dataset is tagged in character-level, subword-level token\n","    label should be aligned with the given unit. Here, we take the first\n","    character label for the token label.\n","    \"\"\"\n","    is_training = dataset_type == \"train\"\n","    if tokenizer_type == \"xlm-sp\":\n","        strip_char = \"▁\"\n","    elif tokenizer_type == \"bert-wp\":\n","        strip_char = \"##\"\n","    else:\n","        raise ValueError(\"This code only supports XLMRobertaTokenizer & BertWordpieceTokenizer\")\n","\n","    examples = []\n","    ori_examples = []\n","    file_path = Path(file_path)\n","    raw_text = file_path.read_text().strip()\n","    raw_docs = re.split(r\"\\n\\t?\\n\", raw_text)\n","    cnt = 0\n","    for doc in raw_docs:\n","        original_clean_tokens = []  # clean tokens (bert clean func)\n","        original_clean_labels = []  # clean labels (bert clean func)\n","        sentence = \"\"\n","        for line in doc.split(\"\\n\"):\n","            if line[:2] == \"##\":\n","                guid = line.split(\"\\t\")[0].replace(\"##\", \"\")\n","                continue\n","            token, tag = line.split(\"\\t\")\n","            sentence += token\n","            if token == \" \":\n","                continue\n","            original_clean_tokens.append(token)\n","            original_clean_labels.append(tag)\n","        # sentence: \"안녕 하세요..\"\n","        # original_clean_labels: [안, 녕, 하, 세, 요, ., .]\n","        sent_words = sentence.split(\" \")\n","        # sent_words: [안녕, 하세요..]\n","        modi_labels = []\n","        char_idx = 0\n","        for word in sent_words:\n","            # 안녕, 하세요\n","            correct_syllable_num = len(word)\n","            tokenized_word = tokenizer.tokenize(word)\n","            # case1: 음절 tokenizer --> [안, ##녕]\n","            # case2: wp tokenizer --> [안녕]\n","            # case3: 음절, wp tokenizer에서 unk --> [unk]\n","            # unk규칙 --> 어절이 통채로 unk로 변환, 단, 기호는 분리\n","            contain_unk = True if tokenizer.unk_token in tokenized_word else False\n","            for i, token in enumerate(tokenized_word):\n","                token = token.replace(strip_char, \"\")\n","                if not token:\n","                    modi_labels.append(\"O\")\n","                    continue\n","                modi_labels.append(original_clean_labels[char_idx])\n","                if not contain_unk:\n","                    char_idx += len(token)\n","            if contain_unk:\n","                char_idx += correct_syllable_num\n","\n","        text_a = sentence  # original sentence\n","        examples.append(InputExample(guid=guid, text_a=text_a, label=modi_labels))\n","        ori_examples.append({\"original_sentence\": text_a, \"original_clean_labels\": original_clean_labels})\n","        cnt += 1\n","    if not is_training:\n","        data = getattr(hparams, \"data\", {})\n","        data[dataset_type] = {\"original_examples\": ori_examples}\n","        setattr(hparams, \"data\", data)\n","        setattr(hparams, \"tokenizer\", tokenizer)\n","    return examples\n","\n","def _convert_features(examples: List[InputExample]) -> List[InputFeatures]:\n","    return convert_examples_to_features(\n","        examples,\n","        tokenizer,\n","        label_list=get_labels(),\n","        max_length=hparams.max_seq_length,\n","        task_mode=\"tagging\",\n","    )\n","\n","def _create_dataset(file_path: str, dataset_type: str) -> TensorDataset:\n","    examples = _create_examples(file_path, dataset_type)\n","    features = _convert_features(examples)\n","\n","    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n","    # Some model does not make use of token type ids (e.g. RoBERTa)\n","    all_token_type_ids = torch.tensor(\n","        [0 if f.token_type_ids is None else f.token_type_ids for f in features], dtype=torch.long\n","    )\n","    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n","\n","    return TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n","\n","\n","_create_dataset(\"./klue-ner-v1.1_train.tsv\",\"train\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uNrW_umOqaAQ"},"source":["from overrides import overrides\n","\n","class BaseTransformer(pl.LightningModule):\n","    \"\"\"Initializes a model, tokenizer and config for the task.\"\"\"\n","\n","    USE_TOKEN_TYPE_MODELS = [\"bert\", \"xlnet\", \"electra\"]\n","\n","    def __init__(\n","        self,\n","        hparams: argparse.Namespace,\n","        num_labels: Optional[int] = None,\n","        mode: str = \"base\",\n","        config: Optional[PretrainedConfig] = None,\n","        model_type: Optional[str] = None,\n","        tokenizer: Optional[PreTrainedTokenizer] = None,\n","        metrics: Dict[str, Any] = {},\n","        **config_kwargs: Dict[str, Any],\n","    ) -> None:\n","        super().__init__()\n","\n","        data = getattr(hparams, \"data\", None)\n","        if data is not None:\n","            delattr(hparams, \"data\")\n","        self.save_hyperparameters(hparams)\n","        self.hparams.data = data\n","\n","        self.step_count = 0\n","        self.output_dir = Path(self.hparams.output_dir)\n","        self.predictions: List[int] = []\n","\n","        cache_dir = self.hparams.cache_dir if self.hparams.cache_dir else None\n","        if config is None:\n","            self.config = AutoConfig.from_pretrained(\n","                self.hparams.config_name if self.hparams.config_name else self.hparams.model_name_or_path,\n","                **({\"num_labels\": num_labels} if num_labels is not None else {}),\n","                cache_dir=cache_dir,\n","                **config_kwargs,\n","            )\n","        else:\n","            self.config: PretrainedConfig = config  # type: ignore[no-redef]\n","\n","        extra_model_params = (\"encoder_layerdrop\", \"decoder_layerdrop\", \"dropout\", \"attention_dropout\")\n","        for p in extra_model_params:\n","            if getattr(self.hparams, p, None):\n","                assert hasattr(self.config, p), f\"model config doesn't have a `{p}` attribute\"\n","                setattr(self.config, p, getattr(self.hparams, p))\n","\n","        if tokenizer is None:\n","            self.tokenizer = AutoTokenizer.from_pretrained(\n","                self.hparams.tokenizer_name if self.hparams.tokenizer_name else self.hparams.model_name_or_path,\n","                cache_dir=cache_dir,\n","            )\n","        else:\n","            self.tokenizer = tokenizer\n","        self.model = model_type.from_pretrained(\n","            self.hparams.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in self.hparams.model_name_or_path),\n","            config=self.config,\n","            cache_dir=cache_dir,\n","        )\n","        self.metrics = nn.ModuleDict(metrics)\n","        self.eval_dataset_type = \"valid\"\n","\n","    def is_use_token_type(self) -> bool:\n","        if self.config.model_type in set(self.USE_TOKEN_TYPE_MODELS):\n","            return True\n","        else:\n","            return False\n","\n","    def get_lr_scheduler(self) -> Any:\n","        get_schedule_func = arg_to_scheduler[self.hparams.lr_scheduler]\n","\n","        scheduler = get_schedule_func(\n","            self.opt, num_warmup_steps=self.num_warmup_steps(), num_training_steps=self.total_steps()\n","        )\n","        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n","        return scheduler\n","\n","    def configure_optimizers(self) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n","        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.hparams.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        if self.hparams.adafactor:\n","            optimizer = Adafactor(\n","                optimizer_grouped_parameters, lr=self.hparams.learning_rate, scale_parameter=False, relative_step=False\n","            )\n","        else:\n","            optimizer = AdamW(\n","                optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon\n","            )\n","        self.opt = optimizer\n","        scheduler = self.get_lr_scheduler()\n","        return [optimizer], [scheduler]\n","\n","    def training_step(self) -> Dict[str, torch.Tensor]:\n","        raise NotImplementedError\n","\n","    def training_step_end(self, training_step_outputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n","        # For DataParallel\n","        return {\"loss\": training_step_outputs[\"loss\"].mean()}\n","\n","    def validation_step(self, batch: List[torch.Tensor], batch_idx: int, data_type: str) -> Dict[str, torch.Tensor]:\n","        # return Format: (e.g. dictionary {\"logits\": logits, \"labels\": labels})\n","        raise NotImplementedError\n","\n","    def validation_epoch_end(\n","        self, outputs: List[Dict[str, torch.Tensor]], data_type: str = \"valid\", write_predictions: bool = False\n","    ) -> None:\n","        preds = self._convert_outputs_to_preds(outputs)\n","        labels = torch.cat([output[\"labels\"] for output in outputs], dim=0)\n","\n","        if write_predictions is True:\n","            self.predictions = preds\n","\n","        self._set_metrics_device()\n","        for k, metric in self.metrics.items():\n","            metric(preds, labels)\n","            self.log(f\"{data_type}/{k}\", metric, on_step=False, on_epoch=True, logger=True)\n","\n","    def test_step(self, batch: List[torch.Tensor], batch_idx: int) -> Dict[str, torch.Tensor]:\n","        assert self.eval_dataset_type in {\"valid\", \"test\"}\n","        return self.validation_step(batch, batch_idx, data_type=self.eval_dataset_type)\n","\n","    def test_epoch_end(self, outputs: List[Dict[str, torch.Tensor]]) -> None:\n","        assert self.eval_dataset_type in {\"valid\", \"test\"}\n","        return self.validation_epoch_end(outputs, data_type=self.eval_dataset_type, write_predictions=True)\n","\n","    def _convert_outputs_to_preds(self, outputs: List[Dict[str, torch.Tensor]]) -> Any:\n","        # outputs is output (dict, return object from validation_step) of list\n","        raise NotImplementedError\n","\n","    def _set_metrics_device(self) -> None:\n","        device = next(self.parameters()).device\n","        for _, metric in self.metrics.items():\n","            if metric.device is None:\n","                metric.device = device\n","\n","    def num_warmup_steps(self) -> Any:\n","        num_warmup_steps = self.hparams.warmup_steps\n","        if num_warmup_steps is None and self.hparams.warmup_ratio is not None:\n","            num_warmup_steps = self.total_steps() * self.hparams.warmup_ratio\n","            num_warmup_steps = math.ceil(num_warmup_steps)\n","\n","        if num_warmup_steps is None:\n","            num_warmup_steps = 0\n","        return num_warmup_steps\n","\n","    def total_steps(self) -> Any:\n","        \"\"\"The number of total training steps that will be run. Used for lr scheduler purposes.\"\"\"\n","        num_devices = max(1, self.hparams.num_gpus)  # TODO: consider num_tpu_cores\n","        effective_batch_size = self.hparams.train_batch_size * self.hparams.accumulate_grad_batches * num_devices\n","        return (self.hparams.dataset_size / effective_batch_size) * self.hparams.max_epochs\n","\n","    @pl.utilities.rank_zero_only\n","    def on_save_checkpoint(self, checkpoint: Dict[str, Any]) -> None:\n","        save_path = self.output_dir.joinpath(\"transformers\")\n","        self.model.config.save_step = self.step_count\n","        self.model.save_pretrained(save_path)\n","        self.tokenizer.save_pretrained(save_path)\n","\n","    @staticmethod\n","    def add_specific_args(parser: argparse.ArgumentParser, root_dir: str) -> argparse.ArgumentParser:\n","        parser.add_argument(\n","            \"--model_name_or_path\",\n","            default=None,\n","            type=str,\n","            required=True,\n","            help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n","        )\n","        parser.add_argument(\n","            \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n","        )\n","        parser.add_argument(\n","            \"--tokenizer_name\",\n","            default=None,\n","            type=str,\n","            help=\"Pretrained tokenizer name or path if not the same as model_name\",\n","        )\n","        parser.add_argument(\n","            \"--cache_dir\",\n","            default=\"\",\n","            type=str,\n","            help=\"Where do you want to store the pre-trained models downloaded from s3\",\n","        )\n","        parser.add_argument(\n","            \"--encoder_layerdrop\",\n","            type=float,\n","            help=\"Encoder layer dropout probability (Optional). Goes into model.config\",\n","        )\n","        parser.add_argument(\n","            \"--decoder_layerdrop\",\n","            type=float,\n","            help=\"Decoder layer dropout probability (Optional). Goes into model.config\",\n","        )\n","        parser.add_argument(\n","            \"--dropout\",\n","            type=float,\n","            help=\"Dropout probability (Optional). Goes into model.config\",\n","        )\n","        parser.add_argument(\n","            \"--attention_dropout\",\n","            type=float,\n","            help=\"Attention dropout probability (Optional). Goes into model.config\",\n","        )\n","        parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n","        parser.add_argument(\n","            \"--lr_scheduler\",\n","            default=\"linear\",\n","            choices=arg_to_scheduler_choices,\n","            metavar=arg_to_scheduler_metavar,\n","            type=str,\n","            help=\"Learning rate scheduler\",\n","        )\n","        parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n","        parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n","        parser.add_argument(\"--warmup_steps\", default=None, type=int, help=\"Linear warmup over warmup_steps.\")\n","        parser.add_argument(\"--warmup_ratio\", default=None, type=float, help=\"Linear warmup over warmup_step ratio.\")\n","        parser.add_argument(\"--num_train_epochs\", dest=\"max_epochs\", default=4, type=int)\n","        parser.add_argument(\"--adafactor\", action=\"store_true\")\n","        parser.add_argument(\"--verbose_step_count\", default=100, type=int)\n","        return parser\n","\n","class NERTransformer(BaseTransformer):\n","\n","    mode = Mode.NamedEntityRecognition\n","\n","    def __init__(self, hparams: Union[Dict[str, Any], argparse.Namespace], metrics: dict = {}) -> None:\n","        if type(hparams) == dict:\n","            hparams = argparse.Namespace(**hparams)\n","\n","        self.tokenizer = hparams.tokenizer\n","        self.tokenizer_type = check_tokenizer_type(self.tokenizer)  # [\"xlm-sp\", \"bert-wp\", \"other']\n","        # When unk, representing subword, is expanded to represent multiple\n","        # characters to align with character-level labels, this special\n","        # representation is used to represent characters from the second.\n","        # (e.g., 찝찝이 [UNK] --> 찝 [UNK] / 찝 [+UNK] / 이 [+UNK])\n","        self.in_unk_token = \"[+UNK]\"\n","\n","        super().__init__(\n","            hparams,\n","            num_labels=hparams.num_labels,\n","            mode=self.mode,\n","            model_type=AutoModelForTokenClassification,\n","            metrics=metrics,\n","        )\n","\n","    @overrides\n","    def forward(self, **inputs: torch.Tensor) -> Any:\n","        return self.model(**inputs)\n","\n","    @overrides\n","    def training_step(self, batch: List[torch.Tensor], batch_idx: int) -> dict:\n","        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n","\n","        if self.is_use_token_type():\n","            inputs[\"token_type_ids\"] = batch[2]\n","        outputs = self(**inputs)\n","        loss = outputs[0]\n","\n","        self.log(\"train/loss\", loss)\n","        return {\"loss\": loss}\n","\n","    @overrides\n","    def validation_step(self, batch: List[torch.Tensor], batch_idx: int, data_type: str = \"valid\") -> dict:\n","        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1], \"labels\": batch[3]}\n","\n","        if self.is_use_token_type():\n","            inputs[\"token_type_ids\"] = batch[2]\n","\n","        outputs = self(**inputs)\n","        loss, logits = outputs[:2]\n","\n","        self.log(f\"{data_type}/loss\", loss, on_step=False, on_epoch=True, logger=True)\n","\n","        return {\"logits\": logits, \"labels\": inputs[\"labels\"]}\n","\n","    @overrides\n","    def validation_epoch_end(\n","        self, outputs: List[Dict[str, torch.Tensor]], data_type: str = \"valid\", write_predictions: bool = False\n","    ) -> None:\n","        \"\"\"When validation step ends, either token- or character-level predicted\n","        labels are aligned with the original character-level labels and then\n","        evaluated.\n","        \"\"\"\n","        list_of_subword_preds = self._convert_outputs_to_preds(outputs)\n","        if self.tokenizer_type == \"xlm-sp\":\n","            strip_char = \"▁\"\n","        elif self.tokenizer_type == \"bert-wp\":\n","            strip_char = \"##\"\n","        else:\n","            raise ValueError(\"This code only supports XLMRobertaTokenizer & BertWordpieceTokenizer\")\n","\n","        original_examples = self.hparams.data[data_type][\"original_examples\"]\n","        list_of_character_preds = []\n","        list_of_originals = []\n","        label_list = self.hparams.label_list\n","\n","        for i, (subword_preds, example) in enumerate(zip(list_of_subword_preds, original_examples)):\n","            original_sentence = example[\"original_sentence\"]  # 안녕 하세요 ^^\n","            character_preds = [subword_preds[0].tolist()]  # [CLS]\n","            character_preds_idx = 1\n","            for word in original_sentence.split(\" \"):  # ['안녕', '하세요', '^^']\n","                if character_preds_idx >= self.hparams.max_seq_length - 1:\n","                    break\n","                subwords = self.tokenizer.tokenize(word)  # 안녕 -> [안, ##녕] / 하세요 -> [하, ##세요] / ^^ -> [UNK]\n","                if self.tokenizer.unk_token in subwords:  # 뻥튀기가 필요한 case!\n","                    unk_aligned_subwords = self.tokenizer_out_aligner(\n","                        word, subwords, strip_char\n","                    )  # [UNK] -> [UNK, +UNK]\n","                    unk_flag = False\n","                    for subword in unk_aligned_subwords:\n","                        if character_preds_idx >= self.hparams.max_seq_length - 1:\n","                            break\n","                        subword_pred = subword_preds[character_preds_idx].tolist()\n","                        subword_pred_label = label_list[subword_pred]\n","                        if subword == self.tokenizer.unk_token:\n","                            unk_flag = True\n","                            character_preds.append(subword_pred)\n","                            continue\n","                        elif subword == self.in_unk_token:\n","                            if subword_pred_label == \"O\":\n","                                character_preds.append(subword_pred)\n","                            else:\n","                                _, entity_category = subword_pred_label.split(\"-\")\n","                                character_pred_label = \"I-\" + entity_category\n","                                character_pred = label_list.index(character_pred_label)\n","                                character_preds.append(character_pred)\n","                            continue\n","                        else:\n","                            if unk_flag:\n","                                character_preds_idx += 1\n","                                subword_pred = subword_preds[character_preds_idx].tolist()\n","                                character_preds.append(subword_pred)\n","                                unk_flag = False\n","                            else:\n","                                character_preds.append(subword_pred)\n","                                character_preds_idx += 1  # `+UNK`가 끝나는 시점에서도 += 1 을 해줘야 다음 label로 넘어감\n","                else:\n","                    for subword in subwords:\n","                        if character_preds_idx >= self.hparams.max_seq_length - 1:\n","                            break\n","                        subword = subword.replace(strip_char, \"\")  # xlm roberta: \"▁\" / others \"##\"\n","                        subword_pred = subword_preds[character_preds_idx].tolist()\n","                        subword_pred_label = label_list[subword_pred]\n","                        for i in range(0, len(subword)):  # 안, 녕\n","                            if i == 0:\n","                                character_preds.append(subword_pred)\n","                            else:\n","                                if subword_pred_label == \"O\":\n","                                    character_preds.append(subword_pred)\n","                                else:\n","                                    _, entity_category = subword_pred_label.split(\"-\")\n","                                    character_pred_label = \"I-\" + entity_category\n","                                    character_pred = label_list.index(character_pred_label)\n","                                    character_preds.append(character_pred)\n","                        character_preds_idx += 1\n","            character_preds.append(subword_preds[-1].tolist())  # [SEP] label\n","            list_of_character_preds.extend(character_preds)\n","            original_labels = [\"O\"] + example[\"original_clean_labels\"][: len(character_preds) - 2] + [\"O\"]\n","            originals = []\n","            for label in original_labels:\n","                originals.append(label_list.index(label))\n","            assert len(character_preds) == len(originals)\n","            list_of_originals.extend(originals)\n","\n","        self._set_metrics_device()\n","\n","        if write_predictions is True:\n","            self.predictions = list_of_character_preds\n","\n","        for k, metric in self.metrics.items():\n","            metric(list_of_character_preds, list_of_originals, label_list)\n","            self.log(f\"{data_type}/{k}\", metric, on_step=False, on_epoch=True, logger=True)\n","\n","    def tokenizer_out_aligner(self, t_in: str, t_out: List[str], strip_char: str = \"##\") -> List[str]:\n","        \"\"\"Aligns with character-level labels after tokenization.\n","        Example:\n","            >>> t_in = \"베쏭이,제5원소\"\n","            >>> t_out = ['[UNK]', ',', '제', '##5', '##원', '##소']\n","            >>> tokenizer_out_aligner(t_in, t_out, strip_char=\"##\")\n","            ['[UNK]', '[+UNK]', '[+UNK]', ',', '제', '##5', '##원', '##소']\n","            >>> t_in = \"미나藤井美菜27가\"\n","            >>> t_out = ['미나', '[UNK]', '[UNK]', '美', '[UNK]', '27', '##가']\n","            >>> tokenizer_out_aligner(t_in, t_out, strip_char=\"##\")\n","            ['미나', '[UNK]', '[UNK]', '美', '[UNK]', '27', '##가']\n","        \"\"\"\n","        t_out_new = []\n","        i, j = 0, 0\n","        UNK_flag = False\n","        while True:\n","            if i == len(t_in) and j == len(t_out) - 1:\n","                break\n","            step_t_out = len(t_out[j].replace(strip_char, \"\")) if t_out[j] != self.tokenizer.unk_token else 1\n","            if UNK_flag:\n","                t_out_new.append(self.in_unk_token)\n","            else:\n","                t_out_new.append(t_out[j])\n","            if j < len(t_out) - 1 and t_out[j] == self.tokenizer.unk_token and t_out[j + 1] != self.tokenizer.unk_token:\n","                i += step_t_out\n","                UNK_flag = True\n","                if t_in[i] == t_out[j + 1][0]:\n","                    j += 1\n","                    UNK_flag = False\n","            else:\n","                i += step_t_out\n","                j += 1\n","                UNK_flag = False\n","            if j == len(t_out):\n","                UNK_flag = True\n","                j -= 1\n","        return t_out_new\n","\n","    @overrides\n","    def _convert_outputs_to_preds(self, outputs: List[Dict[str, torch.Tensor]]) -> torch.Tensor:\n","        logits = torch.cat([output[\"logits\"] for output in outputs], dim=0)\n","        return torch.argmax(logits, axis=2)\n","\n","    @staticmethod\n","    def add_specific_args(parser: argparse.ArgumentParser, root_dir: str) -> argparse.ArgumentParser:\n","        BaseTransformer.add_specific_args(parser, root_dir)\n","        return parser"],"execution_count":null,"outputs":[]}]}